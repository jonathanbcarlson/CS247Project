{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkqTsrVMa_5i",
        "outputId": "6142c4ac-92da-4722-acea-a98739b88023"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Seed:  999\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "manualSeed = 999\n",
        "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "torch.use_deterministic_algorithms(True) # Needed for reproducible results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "21eguJ8Bb37j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch_version = str(torch.__version__)\n",
        "print(f'torch version: {torch_version}')\n",
        "\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "%pip install torch-scatter -f $scatter_src\n",
        "%pip install torch-sparse -f $sparse_src\n",
        "%pip install torch-geometric\n",
        "%pip install ogb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlxyW640fA7W",
        "outputId": "bcc8ab92-147f-4234-d183-754b54dc651b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.1.0+cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_scatter-2.1.2%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt21cu121\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-2.1.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.1.0%2Bcu121/torch_sparse-0.6.18%2Bpt21cu121-cp310-cp310-linux_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.25.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt21cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.3.0)\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.5.0\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.25.2)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.66.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.7)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (2.31.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->ogb) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->outdated>=0.2.0->ogb) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7026 sha256=2e0da9b8f7cec15e2fac0faecc55f08cdd7222001b13db5615367c8805c00590\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount your google drive in google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhVyxWShqpQR",
        "outputId": "56e4a1d5-3a6d-4dcc-97b2-e72803c85fea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_GPU = True\n",
        "\n",
        "if USE_GPU and torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "elif USE_GPU and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlgRisdQZDd0",
        "outputId": "6b97dd11-f069-49ec-d584-38fc7b394993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MOVIE_HEADERS = [\n",
        "    \"movieId\", \"title\", \"releaseDate\", \"videoReleaseDate\", \"IMDb URL\",\n",
        "    \"unknown\", \"Action\", \"Adventure\", \"Animation\", \"Children's\", \"Comedy\",\n",
        "    \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\", \"Horror\",\n",
        "    \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\", \"Thriller\", \"War\", \"Western\"\n",
        "]\n",
        "USER_HEADERS = [\"userId\", \"age\", \"gender\", \"occupation\", \"zipCode\"]\n",
        "RATING_HEADERS = [\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
        "\n",
        "\n",
        "data_path = '/content/drive/MyDrive/CS247/Project/ColdGAN/ml-100k/'\n",
        "\n",
        "# Process user data:\n",
        "df_user = pd.read_csv(\n",
        "    #Path to user data goes here\n",
        "    data_path + 'u.user',\n",
        "    sep='|',\n",
        "    header=None,\n",
        "    names=USER_HEADERS,\n",
        "    index_col='userId',\n",
        "    encoding='ISO-8859-1',\n",
        ")\n",
        "\n",
        "\n",
        "# Process rating data for training:\n",
        "df_rating_train = pd.read_csv(\n",
        "    data_path + 'u1.base',\n",
        "    sep='\\t',\n",
        "    header=None,\n",
        "    index_col='userId',\n",
        "    names=RATING_HEADERS,\n",
        ").reset_index()\n",
        "\n",
        "# Process rating data for testing:\n",
        "df_rating_test = pd.read_csv(\n",
        "    data_path + 'u1.test',\n",
        "    sep='\\t',\n",
        "    header=None,\n",
        "    index_col='userId',\n",
        "    names=RATING_HEADERS,\n",
        ").reset_index()\n",
        "\n",
        "#Combine user and rating data into one vector\n",
        "\n",
        "# df_train = df_user.copy().merge(df_rating_train, how='right', left_on='userId', right_on='userId')\n",
        "\n",
        "# df_test = df_user.copy().merge(df_rating_test, how='right', left_on='userId', right_on='userId')\n",
        "# rating = torch.from_numpy(df_rating_train['rating'].values).to(torch.long)\n"
      ],
      "metadata": {
        "id": "vNXXb2fFh8m-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_mapping = {idx: i for i, idx in enumerate(df_user.index)}\n",
        "\n",
        "age = df_user['age'].values / df_user['age'].values.max()\n",
        "age = torch.from_numpy(age).to(torch.float).view(-1, 1)\n",
        "\n",
        "gender = df_user['gender'].str.get_dummies().values\n",
        "gender = torch.from_numpy(gender).to(torch.float)\n",
        "\n",
        "occupation = df_user['occupation'].str.get_dummies().values\n",
        "occupation = torch.from_numpy(occupation).to(torch.float)\n",
        "\n",
        "zipcode = df_user['zipCode'].str.get_dummies().values\n",
        "zipcode = torch.from_numpy(zipcode).to(torch.float)\n",
        "\n",
        "users = torch.cat([age, gender, occupation, zipcode], dim=-1).to(device)"
      ],
      "metadata": {
        "id": "Ua178u7vmUYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = []\n",
        "timestamps = []\n",
        "num_movies = 1682\n",
        "for user in user_mapping:\n",
        "  user_ratings_df = df_rating_train.loc[df_rating_train['userId'] == user].sort_values('timestamp')\n",
        "  user_ratings = torch.zeros((num_movies,), dtype=torch.float)\n",
        "  user_timestamps = torch.zeros((num_movies,), dtype=torch.float)\n",
        "  for i, (index, row) in enumerate(user_ratings_df.iterrows()):\n",
        "    user_ratings[row['movieId']-1] = row['rating']\n",
        "    user_timestamps[row['movieId']-1] = i + 1\n",
        "\n",
        "\n",
        "\n",
        "  ratings.append(user_ratings)\n",
        "  timestamps.append(user_timestamps)\n",
        "\n",
        "ratings = torch.from_numpy(np.array(ratings)).to(device)\n",
        "timestamps = torch.from_numpy(np.array(timestamps))"
      ],
      "metadata": {
        "id": "ZY2xRMB4mwAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movie_popularity = df_rating_train['movieId'].value_counts().sort_index()\n",
        "movie_popularity = movie_popularity.reindex(list(range(1,num_movies+1)),fill_value=0).values\n",
        "movie_popularity = torch.from_numpy(movie_popularity / np.max(movie_popularity)).to(device)"
      ],
      "metadata": {
        "id": "xOWqyWND6R9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(movie_popularity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-fi_3YM7SqB",
        "outputId": "56baa281-6830-479b-8ab9-90c4b2cd1894"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7913, 0.2169, 0.1550,  ..., 0.0021, 0.0021, 0.0021],\n",
            "       dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(ratings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoOkDevmoL3y",
        "outputId": "18722df7-4a0f-43b3-8bc5-75939e25d4f6"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(users))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWELFj8moN0s",
        "outputId": "5139fb35-bb9d-4d0b-992a-d1c49dd9a470"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "943\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(movie_popularity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y-f4qp_6Zdb",
        "outputId": "ad766b3c-6624-431e-b4cf-8de21837937b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1682"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class RatingsDataset(Dataset):\n",
        "    def __init__(self, users, ratings, timestamps):\n",
        "        self.len = len(users)\n",
        "        self.users = users\n",
        "        self.ratings = ratings\n",
        "        self.timestamps = timestamps\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        user_vec = self.users[idx]\n",
        "        ratings_vec = self.ratings[idx]\n",
        "        timestamps_vec = self.timestamps[idx]\n",
        "\n",
        "        return user_vec, ratings_vec, timestamps_vec\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(data):\n",
        "        user_vec = torch.stack([_[0] for _ in data], dim=0)\n",
        "        ratings_vec = torch.stack([_[1] for _ in data], dim=0)\n",
        "        timestamps_vec = torch.stack([_[2] for _ in data], dim=0)\n",
        "        return user_vec, ratings_vec, timestamps_vec"
      ],
      "metadata": {
        "id": "MZo4xrJ4cNUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 41\n",
        "dataset = RatingsDataset(users, ratings, timestamps)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=RatingsDataset.collate_fn)"
      ],
      "metadata": {
        "id": "KAS3-dbu1gRu"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 41\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this\n",
        "#   size using a transformer.\n",
        "image_size = 64\n",
        "\n",
        "# Number of channels in the training images. For color images this is 3\n",
        "nc = 3\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 100\n",
        "\n",
        "# Size of feature maps in generator\n",
        "ngf = 64\n",
        "\n",
        "# Size of feature maps in discriminator\n",
        "ndf = 64\n",
        "\n",
        "# Number of training epochs\n",
        "num_epochs = 100\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparameter for Adam optimizers\n",
        "beta1 = 0.5\n"
      ],
      "metadata": {
        "id": "je3_b7cUojqx"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get batch data from training set\n",
        "def get_batch_data(file, index, size):  # 1,5->1,2,3,4,5\n",
        "    user = []\n",
        "    item = []\n",
        "    label = []\n",
        "    for i in range(index, index + size):\n",
        "        line = linecache.getline(file, i)\n",
        "        line = line.strip()\n",
        "        line = line.split()\n",
        "        user.append(int(line[0]))\n",
        "        user.append(int(line[0]))\n",
        "        item.append(int(line[1]))\n",
        "        item.append(int(line[2]))\n",
        "        label.append(1.)\n",
        "        label.append(0.)\n",
        "    return user, item, label\n",
        "\n",
        "def file_len(fname):\n",
        "    with open(fname) as f:\n",
        "        for i, l in enumerate(f):\n",
        "            pass\n",
        "    return i + 1\n",
        "\n",
        "\n",
        "# Get category of items\n",
        "def get_category(file_in):\n",
        "    category = {}\n",
        "    #with open(file_in) as fin:\n",
        "    with open(file_in,encoding='unicode_escape') as fin:\n",
        "        for line in fin:\n",
        "            line = line.split('|')\n",
        "            iid = int(line[0]) - 1  # item id starts from 0\n",
        "            category[iid] = line[6:24]\n",
        "    return category\n",
        "\n",
        "\n",
        "# Get training/testing data\n",
        "def get_train_test_data(file_in):\n",
        "    # only record user-item pairs with rating >=4\n",
        "    user_item = {}\n",
        "    with open(file_in) as fin:\n",
        "        for line in fin:\n",
        "            line = line.split()\n",
        "            uid = int(line[0])\n",
        "            iid = int(line[1])\n",
        "            r = float(line[2])\n",
        "            if uid in user_item:\n",
        "                user_item[uid].append(iid)\n",
        "            else:\n",
        "                user_item[uid] = [iid]\n",
        "    return user_item\n",
        "\n",
        "\n",
        "def precision_at_k(r, k):\n",
        "    \"\"\"Score is precision @ k\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "    Returns:\n",
        "        Precision @ k\n",
        "    Raises:\n",
        "        ValueError: len(r) must be >= k\n",
        "    \"\"\"\n",
        "    assert k >= 1\n",
        "    r = np.asarray(r)[:k]\n",
        "    return np.mean(r)\n",
        "\n",
        "\n",
        "def average_precision(r):\n",
        "    \"\"\"Score is average precision (area under PR curve)\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "    Returns:\n",
        "        Average precision\n",
        "    \"\"\"\n",
        "    r = np.asarray(r)\n",
        "    out = [precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n",
        "    if not out:\n",
        "        return 0.\n",
        "    return np.mean(out)\n",
        "\n",
        "\n",
        "def mean_average_precision(rs):\n",
        "    \"\"\"Score is mean average precision\n",
        "    Relevance is binary (nonzero is relevant).\n",
        "    Returns:\n",
        "        Mean average precision\n",
        "    \"\"\"\n",
        "    return np.mean([average_precision(r) for r in rs])\n",
        "\n",
        "\n",
        "def dcg_at_k(r, k):\n",
        "    \"\"\"Score is discounted cumulative gain (dcg)\n",
        "    Relevance is positive real values.  Can use binary\n",
        "    as the previous methods.\n",
        "    Returns:\n",
        "        Discounted cumulative gain\n",
        "    \"\"\"\n",
        "    r = np.asfarray(r)[:k]\n",
        "    if r.size:\n",
        "        # if method == 0:\n",
        "        #     return r[0] + np.sum(r[1:] / np.log2(np.arange(2, r.size + 1)))\n",
        "        # elif method == 1:\n",
        "            return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
        "        # else:\n",
        "        #     raise ValueError('method must be 0 or 1.')\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def ndcg_at_k(r, k):\n",
        "    \"\"\"Score is normalized discounted cumulative gain (ndcg)\n",
        "    Relevance is positive real values.  Can use binary\n",
        "    as the previous methods.\n",
        "    Returns:\n",
        "        Normalized discounted cumulative gain\n",
        "    \"\"\"\n",
        "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
        "    if not dcg_max:\n",
        "        return 0.\n",
        "    return dcg_at_k(r, k) / dcg_max\n",
        "\n",
        "\n",
        "def recall_at_k(r, k, all_pos_num):\n",
        "    r = np.asfarray(r)[:k]\n",
        "    return np.sum(r) / all_pos_num\n",
        "\n",
        "\n",
        "def F1(pre, rec):\n",
        "    if pre + rec > 0:\n",
        "        return (2.0 * pre * rec) / (pre + rec)\n",
        "    else:\n",
        "        return 0.\n",
        "\n",
        "\n",
        "def diversity_by_category(selected_items, item_cate, cate_num):\n",
        "    cate = []\n",
        "    for iid in selected_items:\n",
        "        try:\n",
        "            cate.append(item_cate[iid])\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    cate_count = np.count_nonzero(np.sum(np.asarray(cate, np.float32), axis=0))\n",
        "\n",
        "    return cate_count/cate_num\n",
        "\n",
        "\n",
        "def get_div_train_data(file_in):\n",
        "    user_train_samples = {}\n",
        "\n",
        "    with open(file_in) as fin:\n",
        "        for line in fin:\n",
        "            line = line.split('\\t')\n",
        "            uid = int(line[0])\n",
        "            items = list(map(int, line[1:]))\n",
        "            if uid in user_train_samples:\n",
        "                user_train_samples[uid].append(items)\n",
        "            else:\n",
        "                user_train_samples[uid] = [items]\n",
        "\n",
        "    return user_train_samples\n",
        "\n",
        "\n",
        "def generate_pairwise_diversity_training_data(file_train, file_cate, file_out_pos, file_out_neg, user_num):\n",
        "    pos_data = []  # data for output\n",
        "    neg_data = []\n",
        "\n",
        "    #########################################################################################\n",
        "    # Load data\n",
        "    #########################################################################################\n",
        "    category = get_category(file_cate)\n",
        "    user_item = get_train_test_data(file_train)\n",
        "\n",
        "    # for each user, generate diversity set\n",
        "    for i in range(0, user_num):\n",
        "        uid = i;\n",
        "        print('user:', uid)\n",
        "        try:\n",
        "            items = user_item[uid]\n",
        "        except KeyError:\n",
        "            pass\n",
        "        # the number of trials for each user is set to be the number of viewed items\n",
        "        for j in range(0, len(items)):\n",
        "            first_item = items[j]\n",
        "            pos_div_set = [first_item]  # make sure each viewed item is sampled\n",
        "            pos_cate = [category[first_item]]\n",
        "            num_cate = np.count_nonzero(np.sum(np.asarray(pos_cate, np.float32), axis=0))\n",
        "            # the number of trials for each diversity set is the number of viewed items\n",
        "            for k in range(0, len(items)):\n",
        "                new_item = np.random.choice(items)\n",
        "                try:\n",
        "                    pos_cate.append(category[new_item])\n",
        "                    new_num_cate = np.count_nonzero(np.sum(np.asarray(pos_cate, np.float32), axis=0))\n",
        "                    if new_num_cate - num_cate > 0:\n",
        "                        pos_div_set.append(new_item)\n",
        "                        num_cate = new_num_cate\n",
        "                    if len(pos_div_set) == 10:\n",
        "                        break;\n",
        "                except KeyError:\n",
        "                    pass\n",
        "\n",
        "            pos_div_set.sort()\n",
        "            pos_data.append(str(uid) + '\\t' + '\\t'.join(str(x) for x in pos_div_set))\n",
        "\n",
        "            neg_div_set = [first_item]  # make sure each viewed item is sampled\n",
        "            neg_cate = np.asarray(category[first_item], np.int32).nonzero()[0]\n",
        "            # the number of trials for each diversity set is the number of viewed items\n",
        "            for k in range(0, len(items)):\n",
        "                new_item = items[k]\n",
        "                try:\n",
        "                    if new_item not in neg_div_set:\n",
        "                        new_cate = np.asarray(category[new_item], np.int32).nonzero()[0]\n",
        "                        if np.array_equal(neg_cate, new_cate):\n",
        "                            neg_div_set = np.append(neg_div_set, new_item)\n",
        "                            if len(neg_div_set) > 10:  # due to tensorflow bug\n",
        "                                neg_div_set = np.random.choice(neg_div_set, 10, replace=False)\n",
        "                except KeyError:\n",
        "                    pass\n",
        "            neg_div_set.sort()\n",
        "            neg_data.append(str(uid) + '\\t' + '\\t'.join(str(x) for x in neg_div_set))\n",
        "\n",
        "    with open(file_out_pos, 'w')as fout:\n",
        "        fout.write('\\n'.join(pos_data))\n",
        "\n",
        "    with open(file_out_neg, 'w')as fout:\n",
        "        fout.write('\\n'.join(neg_data))\n"
      ],
      "metadata": {
        "id": "0PD0ahrZopKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# custom weights initialization called on ``netG`` and ``netD``\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "PsYRXUjlorFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generator Code\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Generator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            *block(num_movies, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, num_movies)\n",
        "        )\n",
        "\n",
        "    def forward(self, user_vec, rating_vec):\n",
        "        return self.main(rating_vec)"
      ],
      "metadata": {
        "id": "-9mEA4iXoryi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ngpu = 1"
      ],
      "metadata": {
        "id": "6C83GYP-2ZdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generator\n",
        "netG = Generator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "# if (device == 'cuda') and (ngpu > 1):\n",
        "#     netG = nn.DataParallel(netG, list(range(ngpu)))\n",
        "\n",
        "# # Apply the ``weights_init`` function to randomly initialize all weights\n",
        "# #  to ``mean=0``, ``stdev=0.02``.\n",
        "# netG.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netG)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA5kYr0zot3o",
        "outputId": "24222020-5f4d-4b8d-b3e5-d567d9c8402f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=1682, out_features=128, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
            "    (3): BatchNorm1d(256, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Linear(in_features=256, out_features=512, bias=True)\n",
            "    (6): BatchNorm1d(512, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): ReLU(inplace=True)\n",
            "    (8): Linear(in_features=512, out_features=1024, bias=True)\n",
            "    (9): BatchNorm1d(1024, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): ReLU(inplace=True)\n",
            "    (11): Linear(in_features=1024, out_features=1682, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.ngpu = ngpu\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(num_movies, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, user_vec, rating_vec):\n",
        "        return self.main(rating_vec)"
      ],
      "metadata": {
        "id": "WxnF5N83oxuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Discriminator\n",
        "netD = Discriminator(ngpu).to(device)\n",
        "\n",
        "# Handle multi-GPU if desired\n",
        "# if (device == 'cuda') and (ngpu > 1):\n",
        "#     netD = nn.DataParallel(netD, list(range(ngpu)))\n",
        "\n",
        "# # Apply the ``weights_init`` function to randomly initialize all weights\n",
        "# # like this: ``to mean=0, stdev=0.2``.\n",
        "# netD.apply(weights_init)\n",
        "\n",
        "# Print the model\n",
        "print(netD)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p840enYfozSE",
        "outputId": "d6d7c979-5b12-4c2b-e11e-36d6abf75e71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Linear(in_features=1682, out_features=512, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Linear(in_features=256, out_features=1, bias=True)\n",
            "    (5): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the ``BCELoss`` function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors that we will use to visualize\n",
        "#  the progression of the generator\n",
        "# fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels during training\n",
        "real_label = 1.\n",
        "fake_label = 0.\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
      ],
      "metadata": {
        "id": "iz7Lxq5Io2K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate cold state from warm state\n",
        "p_min = 0.0\n",
        "p_max = 0.5\n",
        "alpha = 1\n",
        "def rejuvenation_function(rating_vector, timestamp_vector, alpha):\n",
        "  #Need time + popularity of item rating\n",
        "  #For tth item, probability of choosing in cold state is\n",
        "  #pm(t) = p_min + (p_max - p_min) * exp(-alpha * [t - pop(i_t)]/ [count(wm)])\n",
        "  count = np.count_nonzero(rating_vector)\n",
        "  t_vector = timestamp_vector\n",
        "  prob_vector = p_min + (p_max - p_min) * np.exp(-alpha * (t_vector - movie_popularity)/ count)\n",
        "  random_selection = torch.from_numpy(np.random.rand(num_movies)).to(device) < prob_vector\n",
        "  return rating_vector * random_selection"
      ],
      "metadata": {
        "id": "VqZ5I56ko-RF"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warm_vector = ratings[0]\n",
        "cold_vector = rejuvenation_function(ratings[0], timestamps[0], alpha)\n",
        "for i in range(num_movies):\n",
        "  if warm_vector[i] != cold_vector[i]:\n",
        "    print(warm_vector[i])\n",
        "    print(cold_vector[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De4TElkjKXK9",
        "outputId": "86b1b242-c7ae-40af-bfde-06e907862a5a"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(2.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(2.)\n",
            "tensor(0.)\n",
            "tensor(2.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(2.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(2.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(2.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(3.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(2.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(4.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(5.)\n",
            "tensor(0.)\n",
            "tensor(2.)\n",
            "tensor(0.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relevant_loss(rating_vector, actual_warm):\n",
        "  #Sum of binary cross-entropy loss b/w sigmoid gan out and wrel\n",
        "  #divided by number items rated by user\n",
        "  n_m = torch.from_numpy(np.count_nonzero(actual_warm, axis=1)).view(-1, 1)\n",
        "  avg_rating = torch.from_numpy(np.average(actual_warm, axis=1)).view(-1, 1)\n",
        "  relevance_vector = (actual_warm > avg_rating).float()\n",
        "  activation_vector = (actual_warm > 0).float()\n",
        "\n",
        "  bce_loss = nn.functional.binary_cross_entropy(\n",
        "      nn.functional.sigmoid(rating_vector), relevance_vector, reduction='none')\n",
        "  loss = torch.sum(bce_loss * activation_vector) / n_m\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "D5Mh9XYhrtcX"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataloader:\n",
        "  cold = rejuvenation_function(i[1], i[2], alpha)\n",
        "  print(relevant_loss(cold, i[1]).sum())\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgI1TGqpUmwU",
        "outputId": "846b9ed5-14be-4fa4-b111-ae9fc88b62af"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1541.2250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "\n",
        "# Lists to keep track of progress\n",
        "fake_warm = []\n",
        "G_losses = []\n",
        "D_losses = []\n",
        "iters = 0\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "print(dataloader)\n",
        "# For each epoch\n",
        "for epoch in range(num_epochs):\n",
        "    # For each batch in the dataloader\n",
        "    for i, data in enumerate(dataloader):\n",
        "        user_vecs = data[0]\n",
        "        ratings_vecs = data[1]\n",
        "        timestamps_vecs = data[2]\n",
        "        ############################\n",
        "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "        ###########################\n",
        "        ## Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        # Format batch\n",
        "        b_size = user_vecs.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        # Forward pass real batch through D\n",
        "        output = netD(user_vecs, ratings_vecs).view(-1)\n",
        "        # Calculate loss on all-real batch\n",
        "        errD_real = criterion(output, label)\n",
        "        # Calculate gradients for D in backward pass\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        ## Train with all-fake batch\n",
        "        # Generate batch of latent vectors\n",
        "        cold_vectors = rejuvenation_function(ratings_vecs, timestamps_vecs, alpha)\n",
        "        # Generate fake image batch with G\n",
        "        fake  = netG(user_vecs, cold_vectors)\n",
        "        label.fill_(fake_label)\n",
        "        # Classify all fake batch with D\n",
        "        output = netD(user_vecs, fake).view(-1)\n",
        "        # Calculate D's loss on the all-fake batch\n",
        "        errD_fake = criterion(output, label)\n",
        "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        # Compute error of D as sum over the fake and the real batches\n",
        "        errD = errD_real + errD_fake\n",
        "        # Update D\n",
        "        optimizerD.step()\n",
        "\n",
        "        ############################\n",
        "        # (2) Update G network: maximize log(D(G(z)))\n",
        "        ###########################\n",
        "        #Generate fakes again\n",
        "        fake  = netG(user_vecs, cold_vectors)\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label)  # fake labels are real for generator cost\n",
        "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "        output = netD(user_vecs, fake).view(-1)\n",
        "        # Calculate G's loss based on this output\n",
        "        rel_loss = 0 #relevant_loss(fake, ratings_vecs).sum() / np.count_nonzero(ratings_vecs)\n",
        "        errG = criterion(output, label) + rel_loss\n",
        "        # Calculate gradients for G\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        # Update G\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Save Losses for plotting later\n",
        "        G_losses.append(errG.item())\n",
        "        D_losses.append(errD.item())\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            #Get random cold vector\n",
        "            idx = np.random.randint(len(users), size=2)\n",
        "            noise_user = users[idx]\n",
        "            fixed_noise = rejuvenation_function(ratings[idx], timestamps[idx], alpha=0.1)\n",
        "            with torch.no_grad():\n",
        "                fake = netG(noise_user, fixed_noise)\n",
        "            fake_warm.append(fake)\n",
        "\n",
        "        iters += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8ZixgF6o4Iw",
        "outputId": "9738284b-7ccd-4cbb-df49-20308a43481b"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training Loop...\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7abccc0cd990>\n",
            "[0/100][0/23]\tLoss_D: 1.4190\tLoss_G: 0.7234\tD(x): 0.4701\tD(G(z)): 0.4851 / 0.4851\n",
            "[1/100][0/23]\tLoss_D: 1.4240\tLoss_G: 0.7234\tD(x): 0.4679\tD(G(z)): 0.4851 / 0.4851\n",
            "[2/100][0/23]\tLoss_D: 1.4188\tLoss_G: 0.7234\tD(x): 0.4702\tD(G(z)): 0.4851 / 0.4851\n",
            "[3/100][0/23]\tLoss_D: 1.4249\tLoss_G: 0.7234\tD(x): 0.4675\tD(G(z)): 0.4851 / 0.4851\n",
            "[4/100][0/23]\tLoss_D: 1.4221\tLoss_G: 0.7234\tD(x): 0.4688\tD(G(z)): 0.4851 / 0.4851\n",
            "[5/100][0/23]\tLoss_D: 1.4296\tLoss_G: 0.7234\tD(x): 0.4651\tD(G(z)): 0.4851 / 0.4851\n",
            "[6/100][0/23]\tLoss_D: 1.4256\tLoss_G: 0.7234\tD(x): 0.4671\tD(G(z)): 0.4851 / 0.4851\n",
            "[7/100][0/23]\tLoss_D: 1.4155\tLoss_G: 0.7234\tD(x): 0.4718\tD(G(z)): 0.4851 / 0.4851\n",
            "[8/100][0/23]\tLoss_D: 1.4179\tLoss_G: 0.7234\tD(x): 0.4706\tD(G(z)): 0.4851 / 0.4851\n",
            "[9/100][0/23]\tLoss_D: 1.4301\tLoss_G: 0.7234\tD(x): 0.4651\tD(G(z)): 0.4851 / 0.4851\n",
            "[10/100][0/23]\tLoss_D: 1.4206\tLoss_G: 0.7234\tD(x): 0.4694\tD(G(z)): 0.4851 / 0.4851\n",
            "[11/100][0/23]\tLoss_D: 1.4171\tLoss_G: 0.7234\tD(x): 0.4710\tD(G(z)): 0.4851 / 0.4851\n",
            "[12/100][0/23]\tLoss_D: 1.4193\tLoss_G: 0.7234\tD(x): 0.4700\tD(G(z)): 0.4851 / 0.4851\n",
            "[13/100][0/23]\tLoss_D: 1.4246\tLoss_G: 0.7234\tD(x): 0.4675\tD(G(z)): 0.4851 / 0.4851\n",
            "[14/100][0/23]\tLoss_D: 1.4225\tLoss_G: 0.7233\tD(x): 0.4686\tD(G(z)): 0.4851 / 0.4851\n",
            "[15/100][0/23]\tLoss_D: 1.4147\tLoss_G: 0.7234\tD(x): 0.4722\tD(G(z)): 0.4851 / 0.4851\n",
            "[16/100][0/23]\tLoss_D: 1.4159\tLoss_G: 0.7234\tD(x): 0.4715\tD(G(z)): 0.4851 / 0.4851\n",
            "[17/100][0/23]\tLoss_D: 1.4223\tLoss_G: 0.7233\tD(x): 0.4687\tD(G(z)): 0.4851 / 0.4851\n",
            "[18/100][0/23]\tLoss_D: 1.4253\tLoss_G: 0.7234\tD(x): 0.4673\tD(G(z)): 0.4851 / 0.4851\n",
            "[19/100][0/23]\tLoss_D: 1.4207\tLoss_G: 0.7234\tD(x): 0.4695\tD(G(z)): 0.4851 / 0.4851\n",
            "[20/100][0/23]\tLoss_D: 1.4165\tLoss_G: 0.7234\tD(x): 0.4713\tD(G(z)): 0.4851 / 0.4851\n",
            "[21/100][0/23]\tLoss_D: 1.4201\tLoss_G: 0.7233\tD(x): 0.4696\tD(G(z)): 0.4851 / 0.4851\n",
            "[22/100][0/23]\tLoss_D: 1.4224\tLoss_G: 0.7234\tD(x): 0.4686\tD(G(z)): 0.4851 / 0.4851\n",
            "[23/100][0/23]\tLoss_D: 1.4207\tLoss_G: 0.7234\tD(x): 0.4694\tD(G(z)): 0.4851 / 0.4851\n",
            "[24/100][0/23]\tLoss_D: 1.4230\tLoss_G: 0.7234\tD(x): 0.4683\tD(G(z)): 0.4851 / 0.4851\n",
            "[25/100][0/23]\tLoss_D: 1.4266\tLoss_G: 0.7234\tD(x): 0.4666\tD(G(z)): 0.4851 / 0.4851\n",
            "[26/100][0/23]\tLoss_D: 1.4185\tLoss_G: 0.7233\tD(x): 0.4705\tD(G(z)): 0.4851 / 0.4851\n",
            "[27/100][0/23]\tLoss_D: 1.4245\tLoss_G: 0.7233\tD(x): 0.4677\tD(G(z)): 0.4851 / 0.4851\n",
            "[28/100][0/23]\tLoss_D: 1.4203\tLoss_G: 0.7234\tD(x): 0.4695\tD(G(z)): 0.4851 / 0.4851\n",
            "[29/100][0/23]\tLoss_D: 1.4249\tLoss_G: 0.7234\tD(x): 0.4673\tD(G(z)): 0.4851 / 0.4851\n",
            "[30/100][0/23]\tLoss_D: 1.4274\tLoss_G: 0.7233\tD(x): 0.4663\tD(G(z)): 0.4851 / 0.4851\n",
            "[31/100][0/23]\tLoss_D: 1.4240\tLoss_G: 0.7234\tD(x): 0.4679\tD(G(z)): 0.4851 / 0.4851\n",
            "[32/100][0/23]\tLoss_D: 1.4140\tLoss_G: 0.7234\tD(x): 0.4724\tD(G(z)): 0.4851 / 0.4851\n",
            "[33/100][0/23]\tLoss_D: 1.4192\tLoss_G: 0.7234\tD(x): 0.4702\tD(G(z)): 0.4851 / 0.4851\n",
            "[34/100][0/23]\tLoss_D: 1.4119\tLoss_G: 0.7234\tD(x): 0.4735\tD(G(z)): 0.4851 / 0.4851\n",
            "[35/100][0/23]\tLoss_D: 1.4236\tLoss_G: 0.7233\tD(x): 0.4682\tD(G(z)): 0.4851 / 0.4851\n",
            "[36/100][0/23]\tLoss_D: 1.4180\tLoss_G: 0.7234\tD(x): 0.4706\tD(G(z)): 0.4851 / 0.4851\n",
            "[37/100][0/23]\tLoss_D: 1.4211\tLoss_G: 0.7233\tD(x): 0.4693\tD(G(z)): 0.4851 / 0.4851\n",
            "[38/100][0/23]\tLoss_D: 1.4203\tLoss_G: 0.7233\tD(x): 0.4695\tD(G(z)): 0.4851 / 0.4851\n",
            "[39/100][0/23]\tLoss_D: 1.4239\tLoss_G: 0.7234\tD(x): 0.4679\tD(G(z)): 0.4851 / 0.4851\n",
            "[40/100][0/23]\tLoss_D: 1.4157\tLoss_G: 0.7234\tD(x): 0.4717\tD(G(z)): 0.4851 / 0.4851\n",
            "[41/100][0/23]\tLoss_D: 1.4299\tLoss_G: 0.7234\tD(x): 0.4652\tD(G(z)): 0.4851 / 0.4851\n",
            "[42/100][0/23]\tLoss_D: 1.4232\tLoss_G: 0.7234\tD(x): 0.4682\tD(G(z)): 0.4851 / 0.4851\n",
            "[43/100][0/23]\tLoss_D: 1.4204\tLoss_G: 0.7234\tD(x): 0.4696\tD(G(z)): 0.4851 / 0.4851\n",
            "[44/100][0/23]\tLoss_D: 1.4162\tLoss_G: 0.7234\tD(x): 0.4715\tD(G(z)): 0.4851 / 0.4851\n",
            "[45/100][0/23]\tLoss_D: 1.4151\tLoss_G: 0.7234\tD(x): 0.4719\tD(G(z)): 0.4851 / 0.4851\n",
            "[46/100][0/23]\tLoss_D: 1.4257\tLoss_G: 0.7233\tD(x): 0.4673\tD(G(z)): 0.4851 / 0.4851\n",
            "[47/100][0/23]\tLoss_D: 1.4300\tLoss_G: 0.7234\tD(x): 0.4650\tD(G(z)): 0.4851 / 0.4851\n",
            "[48/100][0/23]\tLoss_D: 1.4273\tLoss_G: 0.7234\tD(x): 0.4664\tD(G(z)): 0.4851 / 0.4851\n",
            "[49/100][0/23]\tLoss_D: 1.4219\tLoss_G: 0.7234\tD(x): 0.4689\tD(G(z)): 0.4851 / 0.4851\n",
            "[50/100][0/23]\tLoss_D: 1.4199\tLoss_G: 0.7234\tD(x): 0.4697\tD(G(z)): 0.4851 / 0.4851\n",
            "[51/100][0/23]\tLoss_D: 1.4168\tLoss_G: 0.7234\tD(x): 0.4711\tD(G(z)): 0.4851 / 0.4851\n",
            "[52/100][0/23]\tLoss_D: 1.4218\tLoss_G: 0.7234\tD(x): 0.4688\tD(G(z)): 0.4851 / 0.4851\n",
            "[53/100][0/23]\tLoss_D: 1.4135\tLoss_G: 0.7234\tD(x): 0.4727\tD(G(z)): 0.4851 / 0.4851\n",
            "[54/100][0/23]\tLoss_D: 1.4244\tLoss_G: 0.7234\tD(x): 0.4678\tD(G(z)): 0.4851 / 0.4851\n",
            "[55/100][0/23]\tLoss_D: 1.4241\tLoss_G: 0.7234\tD(x): 0.4678\tD(G(z)): 0.4851 / 0.4851\n",
            "[56/100][0/23]\tLoss_D: 1.4236\tLoss_G: 0.7234\tD(x): 0.4680\tD(G(z)): 0.4851 / 0.4851\n",
            "[57/100][0/23]\tLoss_D: 1.4233\tLoss_G: 0.7234\tD(x): 0.4682\tD(G(z)): 0.4851 / 0.4851\n",
            "[58/100][0/23]\tLoss_D: 1.4213\tLoss_G: 0.7234\tD(x): 0.4691\tD(G(z)): 0.4851 / 0.4851\n",
            "[59/100][0/23]\tLoss_D: 1.4197\tLoss_G: 0.7234\tD(x): 0.4700\tD(G(z)): 0.4851 / 0.4851\n",
            "[60/100][0/23]\tLoss_D: 1.4139\tLoss_G: 0.7234\tD(x): 0.4725\tD(G(z)): 0.4851 / 0.4851\n",
            "[61/100][0/23]\tLoss_D: 1.4245\tLoss_G: 0.7234\tD(x): 0.4677\tD(G(z)): 0.4851 / 0.4851\n",
            "[62/100][0/23]\tLoss_D: 1.4214\tLoss_G: 0.7234\tD(x): 0.4690\tD(G(z)): 0.4851 / 0.4851\n",
            "[63/100][0/23]\tLoss_D: 1.4217\tLoss_G: 0.7234\tD(x): 0.4691\tD(G(z)): 0.4851 / 0.4851\n",
            "[64/100][0/23]\tLoss_D: 1.4177\tLoss_G: 0.7234\tD(x): 0.4707\tD(G(z)): 0.4851 / 0.4851\n",
            "[65/100][0/23]\tLoss_D: 1.4179\tLoss_G: 0.7233\tD(x): 0.4709\tD(G(z)): 0.4851 / 0.4851\n",
            "[66/100][0/23]\tLoss_D: 1.4239\tLoss_G: 0.7234\tD(x): 0.4679\tD(G(z)): 0.4851 / 0.4851\n",
            "[67/100][0/23]\tLoss_D: 1.4228\tLoss_G: 0.7234\tD(x): 0.4683\tD(G(z)): 0.4851 / 0.4851\n",
            "[68/100][0/23]\tLoss_D: 1.4145\tLoss_G: 0.7234\tD(x): 0.4722\tD(G(z)): 0.4851 / 0.4851\n",
            "[69/100][0/23]\tLoss_D: 1.4120\tLoss_G: 0.7234\tD(x): 0.4734\tD(G(z)): 0.4851 / 0.4851\n",
            "[70/100][0/23]\tLoss_D: 1.4138\tLoss_G: 0.7234\tD(x): 0.4725\tD(G(z)): 0.4851 / 0.4851\n",
            "[71/100][0/23]\tLoss_D: 1.4204\tLoss_G: 0.7234\tD(x): 0.4695\tD(G(z)): 0.4851 / 0.4851\n",
            "[72/100][0/23]\tLoss_D: 1.4223\tLoss_G: 0.7234\tD(x): 0.4687\tD(G(z)): 0.4851 / 0.4851\n",
            "[73/100][0/23]\tLoss_D: 1.4253\tLoss_G: 0.7234\tD(x): 0.4673\tD(G(z)): 0.4851 / 0.4851\n",
            "[74/100][0/23]\tLoss_D: 1.4202\tLoss_G: 0.7234\tD(x): 0.4695\tD(G(z)): 0.4851 / 0.4851\n",
            "[75/100][0/23]\tLoss_D: 1.4188\tLoss_G: 0.7234\tD(x): 0.4702\tD(G(z)): 0.4851 / 0.4851\n",
            "[76/100][0/23]\tLoss_D: 1.4150\tLoss_G: 0.7234\tD(x): 0.4720\tD(G(z)): 0.4851 / 0.4851\n",
            "[77/100][0/23]\tLoss_D: 1.4209\tLoss_G: 0.7234\tD(x): 0.4693\tD(G(z)): 0.4851 / 0.4851\n",
            "[78/100][0/23]\tLoss_D: 1.4237\tLoss_G: 0.7234\tD(x): 0.4679\tD(G(z)): 0.4851 / 0.4851\n",
            "[79/100][0/23]\tLoss_D: 1.4105\tLoss_G: 0.7234\tD(x): 0.4740\tD(G(z)): 0.4851 / 0.4851\n",
            "[80/100][0/23]\tLoss_D: 1.4176\tLoss_G: 0.7234\tD(x): 0.4707\tD(G(z)): 0.4851 / 0.4851\n",
            "[81/100][0/23]\tLoss_D: 1.4237\tLoss_G: 0.7234\tD(x): 0.4680\tD(G(z)): 0.4851 / 0.4851\n",
            "[82/100][0/23]\tLoss_D: 1.4128\tLoss_G: 0.7234\tD(x): 0.4730\tD(G(z)): 0.4851 / 0.4851\n",
            "[83/100][0/23]\tLoss_D: 1.4234\tLoss_G: 0.7234\tD(x): 0.4683\tD(G(z)): 0.4851 / 0.4851\n",
            "[84/100][0/23]\tLoss_D: 1.4292\tLoss_G: 0.7234\tD(x): 0.4656\tD(G(z)): 0.4851 / 0.4851\n",
            "[85/100][0/23]\tLoss_D: 1.4254\tLoss_G: 0.7233\tD(x): 0.4672\tD(G(z)): 0.4851 / 0.4851\n",
            "[86/100][0/23]\tLoss_D: 1.4219\tLoss_G: 0.7234\tD(x): 0.4688\tD(G(z)): 0.4851 / 0.4851\n",
            "[87/100][0/23]\tLoss_D: 1.4173\tLoss_G: 0.7234\tD(x): 0.4709\tD(G(z)): 0.4851 / 0.4851\n",
            "[88/100][0/23]\tLoss_D: 1.4078\tLoss_G: 0.7234\tD(x): 0.4753\tD(G(z)): 0.4851 / 0.4851\n",
            "[89/100][0/23]\tLoss_D: 1.4184\tLoss_G: 0.7234\tD(x): 0.4705\tD(G(z)): 0.4851 / 0.4851\n",
            "[90/100][0/23]\tLoss_D: 1.4222\tLoss_G: 0.7234\tD(x): 0.4688\tD(G(z)): 0.4851 / 0.4851\n",
            "[91/100][0/23]\tLoss_D: 1.4194\tLoss_G: 0.7234\tD(x): 0.4699\tD(G(z)): 0.4851 / 0.4851\n",
            "[92/100][0/23]\tLoss_D: 1.4171\tLoss_G: 0.7234\tD(x): 0.4710\tD(G(z)): 0.4851 / 0.4851\n",
            "[93/100][0/23]\tLoss_D: 1.4258\tLoss_G: 0.7233\tD(x): 0.4669\tD(G(z)): 0.4851 / 0.4851\n",
            "[94/100][0/23]\tLoss_D: 1.4240\tLoss_G: 0.7234\tD(x): 0.4678\tD(G(z)): 0.4851 / 0.4851\n",
            "[95/100][0/23]\tLoss_D: 1.4258\tLoss_G: 0.7234\tD(x): 0.4671\tD(G(z)): 0.4851 / 0.4851\n",
            "[96/100][0/23]\tLoss_D: 1.4128\tLoss_G: 0.7234\tD(x): 0.4730\tD(G(z)): 0.4851 / 0.4851\n",
            "[97/100][0/23]\tLoss_D: 1.4226\tLoss_G: 0.7233\tD(x): 0.4686\tD(G(z)): 0.4851 / 0.4851\n",
            "[98/100][0/23]\tLoss_D: 1.4218\tLoss_G: 0.7233\tD(x): 0.4689\tD(G(z)): 0.4851 / 0.4851\n",
            "[99/100][0/23]\tLoss_D: 1.4131\tLoss_G: 0.7234\tD(x): 0.4728\tD(G(z)): 0.4851 / 0.4851\n"
          ]
        }
      ]
    }
  ]
}